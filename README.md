## This repository contains the implementation of the activation functions in python from scratch.

Activation funcions implemented are softmax, relu, leaky relu, sigmoid and tanh.

let us dive in 

Activation functions are used in  neural networks to create non linear relationship, used for binary and multi class classifications.

